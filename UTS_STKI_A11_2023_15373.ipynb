{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firnanda-ra/projek-chatbot/blob/main/UTS_STKI_A11_2023_15373.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfcO-76nWPxB",
        "outputId": "ea01628d-7e7c-4619-91b4-f407d4b00052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Instal Sastrawi untuk stemming dan NLTK untuk stopwords\n",
        "!pip install Sastrawi nltk\n",
        "\n",
        "# Download database stopwords NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kYvcIQXwzxl",
        "outputId": "11860388-21a6-44ff-dc45-0f121d254a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ompHQj28Wne9",
        "outputId": "0437892e-51e3-40b6-9d1f-332512c355ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/requirements.txt’: File exists\n",
            "Semua folder berhasil dibuat di dalam: /content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda\n"
          ]
        }
      ],
      "source": [
        "PROJECT_PATH = '/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda'\n",
        "\n",
        "# Buat semua folder yang dibutuhkan (mirip seperti Sel 2 Anda sebelumnya)\n",
        "!mkdir -p \"{PROJECT_PATH}/data\"\n",
        "!mkdir -p \"{PROJECT_PATH}/src\"\n",
        "!mkdir -p \"{PROJECT_PATH}/app\"\n",
        "!mkdir -p \"{PROJECT_PATH}/notebooks\"\n",
        "!mkdir -p \"{PROJECT_PATH}/reports\"\n",
        "!mkdir -p \"{PROJECT_PATH}/requirements.txt\"\n",
        "\n",
        "print(f\"Semua folder berhasil dibuat di dalam: {PROJECT_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esyVFT0R0ArM",
        "outputId": "a2b54b4e-bdfb-403f-bbcd-b799e79d011f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda\n",
            "app/  data/  notebooks/  reports/  requirements.txt  src/\n"
          ]
        }
      ],
      "source": [
        "PROJECT_PATH = '/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda'\n",
        "%cd \"{PROJECT_PATH}\"\n",
        "!ls -F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxveOLHkW3d1",
        "outputId": "e62cff29-d9ca-40bd-d470-904f1b03a892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "-rw------- 1 root root  472 Nov  2 05:40 01_kos_griya_cendekia.txt\n",
            "-rw------- 1 root root  440 Nov  2 05:41 02_wisma_bintang_pelajar.txt\n",
            "-rw------- 1 root root  406 Nov  2 05:43 03_pondok_mahasiswa_jaya.txt\n",
            "-rw------- 1 root root  430 Nov  2 05:43 04_kos_amanah_putri.txt\n",
            "-rw------- 1 root root  442 Nov  2 05:43 05_graha_mandiri_residence.txt\n",
            "-rw------- 1 root root  378 Nov  2 05:44 06_kos_pelangi.txt\n",
            "-rw------- 1 root root  429 Nov  2 05:44 07_kos_eksklusif_emerald.txt\n",
            "-rw------- 1 root root  388 Nov  2 05:44 08_wisma_melati.txt\n",
            "-rw------- 1 root root  367 Nov  2 05:45 09_kos_dahlia_putra.txt\n",
            "-rw------- 1 root root  419 Nov  2 05:45 10_kos_green_valley.txt\n",
            "-rw------- 1 root root  321 Nov  2 05:46 11_kos_barokah.txt\n",
            "-rw------- 1 root root  323 Nov  2 05:46 12_kos_simple_living.txt\n",
            "-rw------- 1 root root  321 Nov  2 05:46 13_omah_nyaman.txt\n",
            "-rw------- 1 root root  320 Nov  2 05:47 14_paviliun_cendana.txt\n",
            "-rw------- 1 root root  262 Nov  2 05:47 15_kos_sahabat.txt\n",
            "drwx------ 2 root root 4096 Nov  4 07:45 processed\n"
          ]
        }
      ],
      "source": [
        "!ls -l data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "Qo6-ykTdzxWG",
        "outputId": "22d4b58b-1005-4a3b-ce2f-9783146374ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"388pt\" height=\"621pt\"\n viewBox=\"0.00 0.00 387.50 620.99\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 616.99)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-616.99 383.5,-616.99 383.5,4 -4,4\"/>\n<!-- A -->\n<g id=\"node1\" class=\"node\">\n<title>A</title>\n<ellipse fill=\"pink\" stroke=\"pink\" cx=\"97.5\" cy=\"-594.99\" rx=\"55.49\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-591.29\" font-family=\"Times,serif\" font-size=\"14.00\">User (Klien)</text>\n</g>\n<!-- B -->\n<g id=\"node2\" class=\"node\">\n<title>B</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"195,-518.49 0,-518.49 0,-480.49 195,-480.49 195,-518.49\"/>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-503.29\" font-family=\"Times,serif\" font-size=\"14.00\">Query User</text>\n<text text-anchor=\"middle\" x=\"97.5\" y=\"-488.29\" font-family=\"Times,serif\" font-size=\"14.00\">(&quot;kos putra bebas jam malam?&quot;)</text>\n</g>\n<!-- A&#45;&gt;B -->\n<g id=\"edge1\" class=\"edge\">\n<title>A&#45;&gt;B</title>\n<path fill=\"none\" stroke=\"black\" d=\"M97.5,-576.84C97.5,-563.48 97.5,-544.63 97.5,-529\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"101,-528.81 97.5,-518.81 94,-528.81 101,-528.81\"/>\n<text text-anchor=\"middle\" x=\"124\" y=\"-547.79\" font-family=\"Times,serif\" font-size=\"14.00\"> Bertanya</text>\n</g>\n<!-- C -->\n<g id=\"node3\" class=\"node\">\n<title>C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M271.5,-417.34C271.5,-419.98 237.88,-422.12 196.5,-422.12 155.12,-422.12 121.5,-419.98 121.5,-417.34 121.5,-417.34 121.5,-374.39 121.5,-374.39 121.5,-371.75 155.12,-369.61 196.5,-369.61 237.88,-369.61 271.5,-371.75 271.5,-374.39 271.5,-374.39 271.5,-417.34 271.5,-417.34\"/>\n<path fill=\"none\" stroke=\"black\" d=\"M271.5,-417.34C271.5,-414.71 237.88,-412.57 196.5,-412.57 155.12,-412.57 121.5,-414.71 121.5,-417.34\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-399.67\" font-family=\"Times,serif\" font-size=\"14.00\">Sistem STKI (Retrieval)</text>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-384.67\" font-family=\"Times,serif\" font-size=\"14.00\">VSM &#45; src/vsm_ir.py</text>\n</g>\n<!-- B&#45;&gt;C -->\n<g id=\"edge2\" class=\"edge\">\n<title>B&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M115.2,-480.32C128.9,-466.26 148.23,-446.42 164.6,-429.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"167.42,-431.73 171.89,-422.13 162.4,-426.85 167.42,-431.73\"/>\n<text text-anchor=\"middle\" x=\"185.5\" y=\"-443.79\" font-family=\"Times,serif\" font-size=\"14.00\"> 1. Retrieve</text>\n</g>\n<!-- E -->\n<g id=\"node5\" class=\"node\">\n<title>E</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"274,-318.74 119,-318.74 119,-280.74 274,-280.74 274,-318.74\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-303.54\" font-family=\"Times,serif\" font-size=\"14.00\">Top&#45;3 Dokumen Relevan</text>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-288.54\" font-family=\"Times,serif\" font-size=\"14.00\">(Konteks)</text>\n</g>\n<!-- C&#45;&gt;E -->\n<g id=\"edge4\" class=\"edge\">\n<title>C&#45;&gt;E</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-369.64C196.5,-357.13 196.5,-341.98 196.5,-329.08\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-328.87 196.5,-318.87 193,-328.87 200,-328.87\"/>\n<text text-anchor=\"middle\" x=\"245\" y=\"-340.54\" font-family=\"Times,serif\" font-size=\"14.00\"> 2. Rank (Cosine)</text>\n</g>\n<!-- D -->\n<g id=\"node4\" class=\"node\">\n<title>D</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"379.5,-525.99 376.5,-529.99 355.5,-529.99 352.5,-525.99 213.5,-525.99 213.5,-472.99 379.5,-472.99 379.5,-525.99\"/>\n<text text-anchor=\"middle\" x=\"296.5\" y=\"-510.79\" font-family=\"Times,serif\" font-size=\"14.00\">Korpus (15 Dokumen Kos)</text>\n<text text-anchor=\"middle\" x=\"296.5\" y=\"-495.79\" font-family=\"Times,serif\" font-size=\"14.00\">&#45; data/01_kos.txt</text>\n<text text-anchor=\"middle\" x=\"296.5\" y=\"-480.79\" font-family=\"Times,serif\" font-size=\"14.00\">&#45; ...</text>\n</g>\n<!-- D&#45;&gt;C -->\n<g id=\"edge3\" class=\"edge\">\n<title>D&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M271.27,-472.85C258.26,-459.63 242.26,-443.37 228.4,-429.29\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"230.89,-426.82 221.38,-422.15 225.9,-431.73 230.89,-426.82\"/>\n<text text-anchor=\"middle\" x=\"280.5\" y=\"-443.79\" font-family=\"Times,serif\" font-size=\"14.00\"> Di&#45;indeks</text>\n</g>\n<!-- F -->\n<g id=\"node6\" class=\"node\">\n<title>F</title>\n<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"253,-229.74 140,-229.74 140,-225.74 136,-225.74 136,-221.74 140,-221.74 140,-199.74 136,-199.74 136,-195.74 140,-195.74 140,-191.74 253,-191.74 253,-229.74\"/>\n<polyline fill=\"none\" stroke=\"lightblue\" points=\"140,-225.74 144,-225.74 144,-221.74 140,-221.74 \"/>\n<polyline fill=\"none\" stroke=\"lightblue\" points=\"140,-199.74 144,-199.74 144,-195.74 140,-195.74 \"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-214.54\" font-family=\"Times,serif\" font-size=\"14.00\">Generator (LLM)</text>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-199.54\" font-family=\"Times,serif\" font-size=\"14.00\">Gemini 2.5 Pro</text>\n</g>\n<!-- E&#45;&gt;F -->\n<g id=\"edge5\" class=\"edge\">\n<title>E&#45;&gt;F</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-280.71C196.5,-268.93 196.5,-253.3 196.5,-239.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-239.74 196.5,-229.74 193,-239.74 200,-239.74\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-251.54\" font-family=\"Times,serif\" font-size=\"14.00\"> 3. Augment (Konteks)</text>\n</g>\n<!-- G -->\n<g id=\"node7\" class=\"node\">\n<title>G</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"250,-140.74 143,-140.74 143,-104.74 250,-104.74 250,-140.74\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-119.04\" font-family=\"Times,serif\" font-size=\"14.00\">Jawaban Natural</text>\n</g>\n<!-- F&#45;&gt;G -->\n<g id=\"edge6\" class=\"edge\">\n<title>F&#45;&gt;G</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-191.5C196.5,-179.67 196.5,-164.06 196.5,-150.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-150.78 196.5,-140.78 193,-150.78 200,-150.78\"/>\n<text text-anchor=\"middle\" x=\"230\" y=\"-162.54\" font-family=\"Times,serif\" font-size=\"14.00\"> 4. Generate</text>\n</g>\n<!-- H -->\n<g id=\"node8\" class=\"node\">\n<title>H</title>\n<ellipse fill=\"pink\" stroke=\"pink\" cx=\"196.5\" cy=\"-26.87\" rx=\"77.56\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-30.67\" font-family=\"Times,serif\" font-size=\"14.00\">Chatbot (Gradio)</text>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-15.67\" font-family=\"Times,serif\" font-size=\"14.00\">app/chatbot.py</text>\n</g>\n<!-- G&#45;&gt;H -->\n<g id=\"edge7\" class=\"edge\">\n<title>G&#45;&gt;H</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-104.52C196.5,-93.22 196.5,-78.01 196.5,-64.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-63.84 196.5,-53.84 193,-63.84 200,-63.84\"/>\n<text text-anchor=\"middle\" x=\"237\" y=\"-75.54\" font-family=\"Times,serif\" font-size=\"14.00\"> Menampilkan</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7ce0664e64b0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#SOAL 1\n",
        "import graphviz\n",
        "dot = graphviz.Digraph(comment='Arsitektur RAG Chatbot Kos')\n",
        "dot.attr(rankdir='TB')\n",
        "dot.node('A', 'User (Klien)', shape='ellipse', style='filled', color='pink')\n",
        "dot.node('B', 'Query User\\n(\"kos putra bebas jam malam?\")', shape='box')\n",
        "dot.node('C', 'Sistem STKI (Retrieval)\\nVSM - src/vsm_ir.py', shape='cylinder')\n",
        "dot.node('D', 'Korpus (15 Dokumen Kos)\\n- data/01_kos.txt\\n- ...', shape='folder')\n",
        "dot.node('E', 'Top-3 Dokumen Relevan\\n(Konteks)', shape='box', style='dashed')\n",
        "dot.node('F', 'Generator (LLM)\\nGemini 2.5 Pro', shape='component', style='filled', color='lightblue')\n",
        "dot.node('G', 'Jawaban Natural', shape='box')\n",
        "dot.node('H', 'Chatbot (Gradio)\\napp/chatbot.py', shape='ellipse', style='filled', color='pink')\n",
        "\n",
        "dot.edge('A', 'B', label=' Bertanya')\n",
        "dot.edge('B', 'C', label=' 1. Retrieve')\n",
        "dot.edge('D', 'C', label=' Di-indeks')\n",
        "dot.edge('C', 'E', label=' 2. Rank (Cosine)')\n",
        "dot.edge('E', 'F', label=' 3. Augment (Konteks)')\n",
        "dot.edge('F', 'G', label=' 4. Generate')\n",
        "dot.edge('G', 'H', label=' Menampilkan')\n",
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okv7HQ2BW6L1",
        "outputId": "a09573b7-b093-44fc-e56c-f2fe8e34d54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/preprocess.py\n"
          ]
        }
      ],
      "source": [
        "# SOAL 2\n",
        "%%writefile src/preprocess.py\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# --- Inisialisasi Alat ---\n",
        "try:\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    list_stopwords_nltk = set(stopwords.words('indonesian'))\n",
        "    print(\"Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat Sastrawi/NLTK: {e}\")\n",
        "    stemmer = None\n",
        "    list_stopwords_nltk = set()\n",
        "\n",
        "# --- Fungsi-fungsi Preprocessing ---\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    if not list_stopwords_nltk:\n",
        "        return tokens\n",
        "    clean_tokens = [token for token in tokens if token not in list_stopwords_nltk]\n",
        "    return clean_tokens\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    if not stemmer:\n",
        "        return tokens\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "def preprocess_pipeline(text):\n",
        "    \"\"\"\n",
        "    Fungsi pipeline lengkap untuk memproses satu string teks.\n",
        "    \"\"\"\n",
        "    text_clean = clean_text(text)\n",
        "    tokens = tokenize(text_clean)\n",
        "    tokens_stopped = remove_stopwords(tokens)\n",
        "    tokens_stemmed = stem_tokens(tokens_stopped)\n",
        "    return ' '.join(tokens_stemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65cUm9DLW_PN",
        "outputId": "74adaa6c-742f-4faf-890a-f0f71bbb36ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\n",
            "Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\n",
            "--- DEMO PREPROCESSING (SOAL 2) ---\n",
            "--- BEFORE ---\n",
            "Nama: Kos Griya Cendekia\n",
            "Alamat: Jl. Suka Maju No. 10, Tembalang, Semarang\n",
            "Profil: Kos khusus putri, tenang, bersih, dan aman. Ideal untuk mahasiswi.\n",
            "...\n",
            "\n",
            "--- AFTER ---\n",
            "nama kos griya cendekia alamat jl suka maju no tembalang semarang profil kos khusus putri tenang bersih aman ideal mahasiswi harga tipe a ac km rp tip...\n",
            "\n",
            "(Demo selesai. Tidak ada file yang disimpan.)\n"
          ]
        }
      ],
      "source": [
        "import src.preprocess as pp\n",
        "import importlib\n",
        "importlib.reload(pp) # Paksa Colab membaca ulang file .py\n",
        "\n",
        "# --- Tampilkan Demo Before/After ---\n",
        "print(\"--- DEMO PREPROCESSING (SOAL 2) ---\")\n",
        "try:\n",
        "    with open('data/01_kos_griya_cendekia.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    print(\"--- BEFORE ---\")\n",
        "    print(text[:150] + \"...\") # Tampilkan 150 karakter pertama\n",
        "    print(\"\\n--- AFTER ---\")\n",
        "    print(pp.preprocess_pipeline(text)[:150] + \"...\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File 'data/01_kos_griya_cendekia.txt' tidak ditemukan.\")\n",
        "    print(\"Pastikan Anda sudah meng-upload 15 file .txt ke folder data/\")\n",
        "\n",
        "print(\"\\n(Demo selesai. Tidak ada file yang disimpan.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13-pswqlXE0C",
        "outputId": "ecd83598-5d0b-4fea-fcdb-cf158d72125c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/boolean_ir.py\n"
          ]
        }
      ],
      "source": [
        "# SOAL 3\n",
        "%%writefile src/boolean_ir.py\n",
        "# Sel [9] (REVISI - eval.py)\n",
        "# File ini: src/boolean_ir.py\n",
        "import os\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import src.preprocess as pp  # Import modul preprocessing kita\n",
        "\n",
        "# Inisialisasi stemmer (untuk query)\n",
        "try:\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    print(\"Stemmer berhasil dimuat untuk boolean_ir.py.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat Sastrawi: {e}\")\n",
        "    stemmer = None\n",
        "\n",
        "def build_inverted_index(data_dir_path):\n",
        "    inverted_index = {}\n",
        "    vocabulary = set()\n",
        "    all_doc_ids = set()\n",
        "\n",
        "    try:\n",
        "        filenames = os.listdir(data_dir_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Folder {data_dir_path} tidak ditemukan.\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Membangun index dari {len(filenames)} file di {data_dir_path}...\")\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.txt'):\n",
        "            doc_id = filename\n",
        "            all_doc_ids.add(doc_id)\n",
        "            file_path = os.path.join(data_dir_path, filename)\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                original_text = f.read()\n",
        "\n",
        "            processed_text = pp.preprocess_pipeline(original_text)\n",
        "            tokens = processed_text.split()\n",
        "\n",
        "            vocabulary.update(tokens)\n",
        "\n",
        "            for token in tokens:\n",
        "                if token not in inverted_index:\n",
        "                    inverted_index[token] = set()\n",
        "                inverted_index[token].add(doc_id)\n",
        "\n",
        "    return inverted_index, all_doc_ids, vocabulary\n",
        "\n",
        "def get_postings(term, inverted_index):\n",
        "    return inverted_index.get(term, set())\n",
        "\n",
        "def parse_boolean_query(query_str, inverted_index, all_doc_ids):\n",
        "    if not stemmer:\n",
        "        print(\"Error: Stemmer tidak terinisialisasi.\")\n",
        "        return set()\n",
        "\n",
        "    raw_tokens = query_str.lower().split()\n",
        "    if len(raw_tokens) == 3:\n",
        "        term1 = stemmer.stem(raw_tokens[0])\n",
        "        op = raw_tokens[1].upper()\n",
        "        term2 = stemmer.stem(raw_tokens[2])\n",
        "\n",
        "        postings1 = get_postings(term1, inverted_index)\n",
        "        postings2 = get_postings(term2, inverted_index)\n",
        "\n",
        "        if op == 'AND':\n",
        "            return postings1.intersection(postings2)\n",
        "        elif op == 'OR':\n",
        "            return postings1.union(postings2)\n",
        "        elif op == 'NOT':\n",
        "            return postings1.difference(postings2)\n",
        "        else:\n",
        "            return set() # Operator tidak dikenal\n",
        "    else:\n",
        "        processed_tokens = pp.preprocess_pipeline(query_str).split()\n",
        "        if not processed_tokens:\n",
        "            return set()\n",
        "        return get_postings(processed_tokens[0], inverted_index)\n",
        "\n",
        "# <-- Fungsi calculate_metrics SUDAH DIHAPUS DARI SINI -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy2oLMRoXJQo",
        "outputId": "b3e4d3bb-4908-4c4f-b524-2c95a3432f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmer berhasil dimuat untuk boolean_ir.py.\n",
            "Stemmer berhasil dimuat untuk boolean_ir.py.\n",
            "--- MEMBANGUN INVERTED INDEX (SOAL 3) ---\n",
            "Membangun index dari 16 file di data/...\n",
            "Index berhasil dibangun. Total 215 term.\n",
            "\n",
            "--- Uji Coba Parser ---\n",
            "Hasil query 'putri AND malam': {'11_kos_barokah.txt', '13_omah_nyaman.txt', '01_kos_griya_cendekia.txt', '08_wisma_melati.txt', '04_kos_amanah_putri.txt'}\n",
            "\n",
            "--- HASIL UJI WAJIB (EVALUASI) ---\n",
            "\n",
            "Query: 'putri AND malam'\n",
            "  > Hasil Sistem (5 docs): {'11_kos_barokah.txt', '13_omah_nyaman.txt', '01_kos_griya_cendekia.txt', '08_wisma_melati.txt', '04_kos_amanah_putri.txt'}\n",
            "  > Kunci Jawaban (4 docs): {'04_kos_amanah_putri.txt', '13_omah_nyaman.txt', '01_kos_griya_cendekia.txt', '08_wisma_melati.txt'}\n",
            "  > Precision: 0.80\n",
            "  > Recall: 1.00\n",
            "  > F1-Score: 0.89\n",
            "\n",
            "Query: 'parkir AND mobil'\n",
            "  > Hasil Sistem (4 docs): {'02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '10_kos_green_valley.txt', '07_kos_eksklusif_emerald.txt'}\n",
            "  > Kunci Jawaban (4 docs): {'02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '10_kos_green_valley.txt', '07_kos_eksklusif_emerald.txt'}\n",
            "  > Precision: 1.00\n",
            "  > Recall: 1.00\n",
            "  > F1-Score: 1.00\n",
            "\n",
            "Query: 'ac AND dalam'\n",
            "  > Hasil Sistem (0 docs): set()\n",
            "  > Kunci Jawaban (9 docs): {'03_pondok_mahasiswa_jaya.txt', '14_paviliun_cendana.txt', '12_kos_simple_living.txt', '01_kos_griya_cendekia.txt', '10_kos_green_valley.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '04_kos_amanah_putri.txt', '09_kos_dahlia_putra.txt'}\n",
            "  > Precision: 0.00\n",
            "  > Recall: 0.00\n",
            "  > F1-Score: 0.00\n"
          ]
        }
      ],
      "source": [
        "import src.boolean_ir as bool_ir\n",
        "import src.eval as eval_ir  # <-- TAMBAHKAN IMPORT INI\n",
        "import importlib\n",
        "importlib.reload(bool_ir)\n",
        "importlib.reload(eval_ir)\n",
        "\n",
        "print(\"--- MEMBANGUN INVERTED INDEX (SOAL 3) ---\")\n",
        "DATA_DIR = 'data/'\n",
        "index, doc_ids, vocab = bool_ir.build_inverted_index(DATA_DIR)\n",
        "\n",
        "if index:\n",
        "    print(f\"Index berhasil dibangun. Total {len(vocab)} term.\")\n",
        "\n",
        "    print(\"\\n--- Uji Coba Parser ---\")\n",
        "    query1 = \"putri AND malam\"\n",
        "    hasil1 = bool_ir.parse_boolean_query(query1, index, doc_ids)\n",
        "    print(f\"Hasil query '{query1}': {hasil1}\")\n",
        "\n",
        "    print(\"\\n--- HASIL UJI WAJIB (EVALUASI) ---\")\n",
        "    gold_set = {\n",
        "        \"putri AND malam\": {'01_kos_griya_cendekia.txt', '04_kos_amanah_putri.txt', '08_wisma_melati.txt', '13_omah_nyaman.txt'},\n",
        "        \"parkir AND mobil\": {'02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '10_kos_green_valley.txt'},\n",
        "        \"ac AND dalam\": {'01_kos_griya_cendekia.txt', '03_pondok_mahasiswa_jaya.txt', '04_kos_amanah_putri.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '09_kos_dahlia_putra.txt', '10_kos_green_valley.txt', '12_kos_simple_living.txt', '14_paviliun_cendana.txt'}\n",
        "    }\n",
        "    queries_to_test = [\"putri AND malam\", \"parkir AND mobil\", \"ac AND dalam\"]\n",
        "    for query in queries_to_test:\n",
        "        result_set = bool_ir.parse_boolean_query(query, index, doc_ids)\n",
        "        current_gold_set = gold_set.get(query, set())\n",
        "\n",
        "        # --- REVISI: Panggil dari eval_ir ---\n",
        "        precision, recall, f1 = eval_ir.calculate_precision_recall_f1(result_set, current_gold_set)\n",
        "\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(f\"  > Hasil Sistem ({len(result_set)} docs): {result_set}\")\n",
        "        print(f\"  > Kunci Jawaban ({len(current_gold_set)} docs): {current_gold_set}\")\n",
        "        print(f\"  > Precision: {precision:.2f}\")\n",
        "        print(f\"  > Recall: {recall:.2f}\")\n",
        "        print(f\"  > F1-Score: {f1:.2f}\")\n",
        "else:\n",
        "    print(\"\\nGAGAL menjalankan Soal 3 karena index tidak terbangun.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLtHIUEha2ZL",
        "outputId": "0e9e4f2a-56a7-4f45-eb56-327defb9d44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/vsm_ir.py\n"
          ]
        }
      ],
      "source": [
        "# SOAL 4\n",
        "%%writefile src/vsm_ir.py\n",
        "# Sel [12] (REVISI - eval.py)\n",
        "# File ini: src/vsm_ir.py\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import src.preprocess as pp\n",
        "\n",
        "def preprocess_query(query_str):\n",
        "    return pp.preprocess_pipeline(query_str)\n",
        "\n",
        "def build_vsm(data_dir):\n",
        "    corpus = []\n",
        "    doc_ids = []\n",
        "    print(f\"Membangun VSM dari {data_dir}...\")\n",
        "\n",
        "    try:\n",
        "        filenames = sorted(os.listdir(data_dir))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Folder {data_dir} tidak ditemukan.\")\n",
        "        return None, None, None\n",
        "\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(data_dir, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                original_text = f.read()\n",
        "\n",
        "            processed_text = pp.preprocess_pipeline(original_text)\n",
        "            corpus.append(processed_text)\n",
        "            doc_ids.append(filename)\n",
        "\n",
        "    if not corpus:\n",
        "        print(f\"ERROR: Korpus kosong, tidak ada file .txt di {data_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, norm='l2')\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    print(f\"VSM (TF-IDF Matrix) berhasil dibangun dengan shape: {tfidf_matrix.shape}\")\n",
        "    return vectorizer, tfidf_matrix, doc_ids\n",
        "\n",
        "def search_vsm(raw_query, vectorizer, tfidf_matrix, doc_ids, top_k=5):\n",
        "    processed_query = preprocess_query(raw_query)\n",
        "    if not processed_query:\n",
        "        return []\n",
        "\n",
        "    query_vector = vectorizer.transform([processed_query])\n",
        "    cosine_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
        "    scores = cosine_scores.flatten()\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for i in top_indices:\n",
        "        if scores[i] > 0:\n",
        "            results.append({\n",
        "                'doc_id': doc_ids[i],\n",
        "                'score': scores[i]\n",
        "            })\n",
        "    return results\n",
        "\n",
        "def get_snippet(src_file_path, max_chars=120):\n",
        "    try:\n",
        "        with open(src_file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read().replace('\\n', ' ')\n",
        "            return content[:max_chars] + \"...\"\n",
        "    except FileNotFoundError:\n",
        "        return \"[Snippet tidak ditemukan]\"\n",
        "\n",
        "# <-- Fungsi Evaluasi SUDAH DIHAPUS DARI SINI -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnjrOCbva5RS",
        "outputId": "f460c876-4838-439b-b825-693669b8777a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MEMBANGUN VSM (SOAL 4) ---\n",
            "Membangun VSM dari data/...\n",
            "VSM (TF-IDF Matrix) berhasil dibangun dengan shape: (15, 213)\n",
            "\n",
            "--- DEMO PENCARIAN VSM ---\n",
            "Hasil Top-3 untuk query: 'kos putra bebas jam malam'\n",
            "  > Doc: 12_kos_simple_living.txt (Skor: 0.3560)\n",
            "    Snippet: Nama: Kos Simple Living Alamat: Jl. Gajah Raya No. 88, Semarang Profil: Kos putra. Minimalis dan fungsional. Harga: Rp 9...\n",
            "\n",
            "  > Doc: 09_kos_dahlia_putra.txt (Skor: 0.2959)\n",
            "    Snippet: Nama: Kos Dahlia Putra Alamat: Jl. Pawiyatan Luhur No. 22, Semarang Profil: Kos khusus putra. Strategis dekat bengkel da...\n",
            "\n",
            "  > Doc: 11_kos_barokah.txt (Skor: 0.2166)\n",
            "    Snippet: Nama: Kos Barokah Alamat: Jl. Hasanudin No. 30, Semarang Profil: Kos campur (putra lt 1, putri lt 2). Harga: Rp 650.000/...\n",
            "\n",
            "\n",
            "--- HASIL UJI WAJIB (EVALUASI RANKING) ---\n",
            "\n",
            "Query: 'kos putri yang ada jam malam' (k=5)\n",
            "  > Hasil VSM: ['08_wisma_melati.txt', '11_kos_barokah.txt', '13_omah_nyaman.txt', '04_kos_amanah_putri.txt', '01_kos_griya_cendekia.txt']\n",
            "  > Kunci Jawaban: {'04_kos_amanah_putri.txt', '13_omah_nyaman.txt', '01_kos_griya_cendekia.txt', '08_wisma_melati.txt'}\n",
            "  > Precision@5: 0.80\n",
            "  > MAP@5: 0.80\n",
            "\n",
            "Query: 'kos yang boleh parkir mobil' (k=5)\n",
            "  > Hasil VSM: ['10_kos_green_valley.txt', '07_kos_eksklusif_emerald.txt', '02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '15_kos_sahabat.txt']\n",
            "  > Kunci Jawaban: {'02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '10_kos_green_valley.txt', '07_kos_eksklusif_emerald.txt'}\n",
            "  > Precision@5: 0.80\n",
            "  > MAP@5: 1.00\n",
            "\n",
            "Query: 'kos AC kamar mandi dalam' (k=5)\n",
            "  > Hasil VSM: ['12_kos_simple_living.txt', '04_kos_amanah_putri.txt', '15_kos_sahabat.txt', '14_paviliun_cendana.txt', '07_kos_eksklusif_emerald.txt']\n",
            "  > Kunci Jawaban: {'03_pondok_mahasiswa_jaya.txt', '14_paviliun_cendana.txt', '12_kos_simple_living.txt', '01_kos_griya_cendekia.txt', '10_kos_green_valley.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '04_kos_amanah_putri.txt', '09_kos_dahlia_putra.txt'}\n",
            "  > Precision@5: 0.80\n",
            "  > MAP@5: 0.39\n"
          ]
        }
      ],
      "source": [
        "# (UJI VSM)\n",
        "import src.vsm_ir as vsm_ir\n",
        "import src.eval as eval_ir  # <-- TAMBAHKAN IMPORT INI\n",
        "import importlib\n",
        "import os\n",
        "importlib.reload(vsm_ir)\n",
        "importlib.reload(eval_ir)\n",
        "\n",
        "DATA_DIR = 'data/'\n",
        "print(\"--- MEMBANGUN VSM (SOAL 4) ---\")\n",
        "try:\n",
        "    vectorizer, tfidf_matrix, doc_ids = vsm_ir.build_vsm(DATA_DIR)\n",
        "\n",
        "    if vectorizer is None:\n",
        "        print(\"ERROR: VSM gagal dibangun. Cek folder data/\")\n",
        "    else:\n",
        "        print(\"\\n--- DEMO PENCARIAN VSM ---\")\n",
        "        demo_query = \"kos putra bebas jam malam\"\n",
        "        k_demo = 3\n",
        "        results = vsm_ir.search_vsm(demo_query, vectorizer, tfidf_matrix, doc_ids, top_k=k_demo)\n",
        "\n",
        "        print(f\"Hasil Top-{k_demo} untuk query: '{demo_query}'\")\n",
        "        for res in results:\n",
        "            snippet_path = os.path.join(DATA_DIR, res['doc_id'])\n",
        "            snippet = vsm_ir.get_snippet(snippet_path)\n",
        "            print(f\"  > Doc: {res['doc_id']} (Skor: {res['score']:.4f})\")\n",
        "            print(f\"    Snippet: {snippet}\\n\")\n",
        "\n",
        "        print(\"\\n--- HASIL UJI WAJIB (EVALUASI RANKING) ---\")\n",
        "\n",
        "        gold_set = {\n",
        "            \"putri AND malam\": {'01_kos_griya_cendekia.txt', '04_kos_amanah_putri.txt', '08_wisma_melati.txt', '13_omah_nyaman.txt'},\n",
        "            \"parkir AND mobil\": {'02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '10_kos_green_valley.txt'},\n",
        "            \"ac AND dalam\": {'01_kos_griya_cendekia.txt', '03_pondok_mahasiswa_jaya.txt', '04_kos_amanah_putri.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '09_kos_dahlia_putra.txt', '10_kos_green_valley.txt', '12_kos_simple_living.txt', '14_paviliun_cendana.txt'}\n",
        "        }\n",
        "        queries_to_test = {\n",
        "            \"putri AND malam\": \"kos putri yang ada jam malam\",\n",
        "            \"parkir AND mobil\": \"kos yang boleh parkir mobil\",\n",
        "            \"ac AND dalam\": \"kos AC kamar mandi dalam\"\n",
        "        }\n",
        "        k = 5\n",
        "\n",
        "        for bool_query, natural_query in queries_to_test.items():\n",
        "            vsm_results = vsm_ir.search_vsm(natural_query, vectorizer, tfidf_matrix, doc_ids, top_k=k)\n",
        "            result_doc_ids = [res['doc_id'] for res in vsm_results]\n",
        "            current_gold_set = gold_set.get(bool_query, set())\n",
        "\n",
        "            # --- REVISI: Panggil dari eval_ir ---\n",
        "            p_at_k = eval_ir.calculate_precision_at_k(result_doc_ids, current_gold_set, k)\n",
        "            map_at_k = eval_ir.calculate_map_at_k(result_doc_ids, current_gold_set, k)\n",
        "\n",
        "            print(f\"\\nQuery: '{natural_query}' (k={k})\")\n",
        "            print(f\"  > Hasil VSM: {result_doc_ids}\")\n",
        "            print(f\"  > Kunci Jawaban: {current_gold_set}\")\n",
        "            print(f\"  > Precision@{k}: {p_at_k:.2f}\")\n",
        "            print(f\"  > MAP@{k}: {map_at_k:.2f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nGAGAL menjalankan Soal 4.\")\n",
        "    print(\"Pastikan folder 'data/' sudah ada dan berisi file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aFtjMhMzJIN",
        "outputId": "4acc5b1d-19c4-45f4-9cf7-1821e13e1ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/search_engine.py\n"
          ]
        }
      ],
      "source": [
        "# SOAL 5\n",
        "%%writefile src/search_engine.py\n",
        "# File ini: src/search_engine.py\n",
        "# Orchestrator yang menangani CLI\n",
        "import argparse\n",
        "import os\n",
        "import src.preprocess as pp\n",
        "import src.boolean_ir as bool_ir\n",
        "import src.vsm_ir as vsm_ir\n",
        "\n",
        "# --- Definisi Path ---\n",
        "SRC_DIR = 'data/src/'\n",
        "PROCESSED_DIR = 'data/processed/'\n",
        "\n",
        "# --- Fungsi untuk \"Booting\" Engine ---\n",
        "# (Memuat semua model ke memori)\n",
        "def load_models():\n",
        "    print(\"Memuat model... Harap tunggu.\")\n",
        "\n",
        "    # 1. Muat data yang sudah diproses\n",
        "    try:\n",
        "        corpus, doc_ids = vsm_ir.load_processed_docs(PROCESSED_DIR)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Folder {PROCESSED_DIR} tidak ditemukan.\")\n",
        "        print(\"Pastikan Anda sudah menjalankan preprocessing (Soal 02).\")\n",
        "        return None\n",
        "\n",
        "    # 2. Muat Model Boolean\n",
        "    bool_index, all_doc_ids, vocab = bool_ir.build_inverted_index(PROCESSED_DIR)\n",
        "\n",
        "    # 3. Muat Model VSM\n",
        "    vsm_vectorizer, vsm_matrix = vsm_ir.build_vsm(corpus)\n",
        "\n",
        "    models = {\n",
        "        \"boolean\": {\n",
        "            \"index\": bool_index,\n",
        "            \"doc_ids\": all_doc_ids\n",
        "        },\n",
        "        \"vsm\": {\n",
        "            \"vectorizer\": vsm_vectorizer,\n",
        "            \"matrix\": vsm_matrix,\n",
        "            \"doc_ids\": doc_ids # doc_ids dari VSM (terurut)\n",
        "        }\n",
        "    }\n",
        "    print(\"Semua model berhasil dimuat.\")\n",
        "    return models\n",
        "\n",
        "# --- Fungsi Main ---\n",
        "def main():\n",
        "    # 1. Setup Argumen Parser\n",
        "    parser = argparse.ArgumentParser(description=\"Mesin Pencari Kos Sederhana (CLI)\")\n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        type=str,\n",
        "        choices=['boolean', 'vsm'],\n",
        "        default='vsm',\n",
        "        help=\"Model yang digunakan: 'boolean' or 'vsm' (default: vsm)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--k\",\n",
        "        type=int,\n",
        "        default=3,\n",
        "        help=\"Jumlah top-k dokumen yang ingin ditampilkan (default: 3)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--query\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Query pencarian Anda (WAJIB). Gunakan tanda kutip.\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # 2. Muat Model\n",
        "    models = load_models()\n",
        "    if models is None:\n",
        "        return\n",
        "\n",
        "    # 3. Jalankan Model yang Dipilih\n",
        "\n",
        "    print(f\"\\n--- Menjalankan Pencarian ---\")\n",
        "    print(f\"Model : {args.model}\")\n",
        "    print(f\"Query : {args.query}\")\n",
        "    print(f\"Top-k : {args.k}\")\n",
        "    print(\"-----------------------------\\n\")\n",
        "\n",
        "    if args.model == 'boolean':\n",
        "        m = models[\"boolean\"]\n",
        "        # Query boolean harus dalam format \"term1 OP term2\"\n",
        "        results = bool_ir.parse_boolean_query(args.query, m[\"index\"], m[\"doc_ids\"])\n",
        "\n",
        "        print(f\"Ditemukan {len(results)} dokumen (Boolean tidak di-ranking):\")\n",
        "        # Tampilkan K pertama saja\n",
        "        for i, doc_id in enumerate(list(results)[:args.k]):\n",
        "            snippet_path = os.path.join(SRC_DIR, doc_id)\n",
        "            snippet = vsm_ir.get_snippet(snippet_path, 80) # Snippet lebih pendek\n",
        "            print(f\"  {i+1}. {doc_id}\")\n",
        "            print(f\"     Snippet: {snippet}\\n\")\n",
        "\n",
        "    elif args.model == 'vsm':\n",
        "        m = models[\"vsm\"]\n",
        "        results = vsm_ir.search_vsm(\n",
        "            args.query,\n",
        "            m[\"vectorizer\"],\n",
        "            m[\"matrix\"],\n",
        "            m[\"doc_ids\"],\n",
        "            top_k=args.k\n",
        "        )\n",
        "\n",
        "        print(f\"Ditemukan {len(results)} dokumen (VSM di-ranking):\")\n",
        "        for i, res in enumerate(results):\n",
        "            snippet_path = os.path.join(SRC_DIR, res['doc_id'])\n",
        "            snippet = vsm_ir.get_snippet(snippet_path, 120) # Snippet lebih panjang\n",
        "            print(f\"  {i+1}. {res['doc_id']} (Skor: {res['score']:.4f})\")\n",
        "            print(f\"     Snippet: {snippet}\\n\")\n",
        "\n",
        "# Ini adalah entry point standar Python\n",
        "# Kode di dalam if block ini HANYA akan jalan\n",
        "# jika file ini dieksekusi langsung (bukan di-import)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZlrtdZR1-qG",
        "outputId": "18439510-0e03-4067-a31b-66fff50bb58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\n",
            "Stemmer berhasil dimuat untuk boolean_ir.py.\n",
            "Memuat model... Harap tunggu.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/src/search_engine.py\", line 119, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/src/search_engine.py\", line 73, in main\n",
            "    models = load_models()\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/src/search_engine.py\", line 20, in load_models\n",
            "    corpus, doc_ids = vsm_ir.load_processed_docs(PROCESSED_DIR)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'src.vsm_ir' has no attribute 'load_processed_docs'\n",
            "\n",
            "==================================================\n",
            "\n",
            "Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\n",
            "Stemmer berhasil dimuat untuk boolean_ir.py.\n",
            "Memuat model... Harap tunggu.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/src/search_engine.py\", line 119, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/src/search_engine.py\", line 73, in main\n",
            "    models = load_models()\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/drive/My Drive/STKI-UTS-A11.2023.15373-Firnanda/src/search_engine.py\", line 20, in load_models\n",
            "    corpus, doc_ids = vsm_ir.load_processed_docs(PROCESSED_DIR)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'src.vsm_ir' has no attribute 'load_processed_docs'\n"
          ]
        }
      ],
      "source": [
        "# RUN SEARCH ENGINE\n",
        "!python -m src.search_engine --model vsm --k 3 --query \"kos ac kamar mandi dalam\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "!python -m src.search_engine --model boolean --k 3 --query \"ac AND dalam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE5iNk8pQUbh",
        "outputId": "a39c9def-7bee-46ea-f33e-c112defc6375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/eval.py\n"
          ]
        }
      ],
      "source": [
        "# SOAL 5\n",
        "%%writefile src/eval.py\n",
        "# File ini: src/eval.py\n",
        "# Berisi semua fungsi untuk evaluasi STKI (Soal 3, 4, 5)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_precision_recall_f1(result_set, gold_set):\n",
        "    \"\"\"\n",
        "    Menghitung Precision, Recall, dan F1-Score (untuk model Boolean).\n",
        "    \"\"\"\n",
        "    if not isinstance(result_set, set):\n",
        "        result_set = set(result_set)\n",
        "    if not isinstance(gold_set, set):\n",
        "        gold_set = set(gold_set)\n",
        "\n",
        "    true_positives = result_set.intersection(gold_set)\n",
        "\n",
        "    # Precision\n",
        "    precision = 0.0\n",
        "    if len(result_set) > 0:\n",
        "        precision = len(true_positives) / len(result_set)\n",
        "\n",
        "    # Recall\n",
        "    recall = 0.0\n",
        "    if len(gold_set) > 0:\n",
        "        recall = len(true_positives) / len(gold_set)\n",
        "\n",
        "    # F1-Score\n",
        "    f1 = 0.0\n",
        "    if (precision + recall) > 0:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "def calculate_precision_at_k(result_doc_ids, gold_set, k):\n",
        "    \"\"\"\n",
        "    Menghitung Precision@k (untuk model VSM).\n",
        "    \"\"\"\n",
        "    if not gold_set or k == 0:\n",
        "        return 0.0\n",
        "\n",
        "    if not isinstance(gold_set, set):\n",
        "        gold_set = set(gold_set)\n",
        "\n",
        "    result_at_k = result_doc_ids[:k]\n",
        "    true_positives = [doc for doc in result_at_k if doc in gold_set]\n",
        "\n",
        "    return len(true_positives) / k\n",
        "\n",
        "def calculate_map_at_k(result_doc_ids, gold_set, k):\n",
        "    \"\"\"\n",
        "    Menghitung Mean Average Precision (MAP)@k (untuk model VSM).\n",
        "    \"\"\"\n",
        "    if not gold_set:\n",
        "        return 0.0\n",
        "\n",
        "    if not isinstance(gold_set, set):\n",
        "        gold_set = set(gold_set)\n",
        "\n",
        "    precisions = []\n",
        "    relevant_count = 0\n",
        "\n",
        "    for i, doc_id in enumerate(result_doc_ids[:k]):\n",
        "        if doc_id in gold_set:\n",
        "            relevant_count += 1\n",
        "            current_precision = relevant_count / (i + 1)\n",
        "            precisions.append(current_precision)\n",
        "\n",
        "    if not precisions:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_in_gold_set = len(gold_set)\n",
        "    return sum(precisions) / relevant_in_gold_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP-grj16UMih",
        "outputId": "b8ceba90-8f47-4490-c1bf-ac5315de4d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\n",
            "--- EVALUASI SKEMA PEMBOBOTAN (SOAL 5) ---\n",
            "\n",
            "--- Model 1: TF-IDF Standar (Default) ---\n",
            "Query: 'kos putri yang ada jam malam' -> MAP@5: 0.8042\n",
            "Query: 'kos yang boleh parkir mobil' -> MAP@5: 1.0000\n",
            "Query: 'kos AC kamar mandi dalam' -> MAP@5: 0.3944\n",
            "\n",
            "--- Model 2: TF-IDF Sublinear (sublinear_tf=True) ---\n",
            "VSM (Sublinear TF) berhasil dibangun dengan shape: (15, 213)\n",
            "Query: 'kos putri yang ada jam malam' -> MAP@5: 0.6792\n",
            "Query: 'kos yang boleh parkir mobil' -> MAP@5: 1.0000\n",
            "Query: 'kos AC kamar mandi dalam' -> MAP@5: 0.5556\n",
            "\n",
            "==================================================\n",
            "   PERBANDINGAN SKEMA PEMBOBOTAN (Rata-rata MAP@5)\n",
            "==================================================\n",
            "  TF-IDF Standar       : 0.7329\n",
            "  TF-IDF Sublinear     : 0.7449\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import src.vsm_ir as vsm_ir\n",
        "import src.eval as eval_ir\n",
        "import src.preprocess as pp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import os\n",
        "import importlib\n",
        "importlib.reload(vsm_ir)\n",
        "importlib.reload(eval_ir)\n",
        "importlib.reload(pp)\n",
        "\n",
        "print(\"--- EVALUASI SKEMA PEMBOBOTAN (SOAL 5) ---\")\n",
        "\n",
        "# --- 1. Muat Data ---\n",
        "DATA_DIR = 'data/'\n",
        "try:\n",
        "    corpus = []\n",
        "    doc_ids_list = []\n",
        "    for filename in sorted(os.listdir(DATA_DIR)):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(DATA_DIR, filename), 'r', encoding='utf-8') as f:\n",
        "                corpus.append(pp.preprocess_pipeline(f.read()))\n",
        "                doc_ids_list.append(filename)\n",
        "    if not corpus:\n",
        "         raise FileNotFoundError\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Folder {DATA_DIR} tidak ditemukan/kosong.\")\n",
        "    raise SystemExit(\"Data korpus tidak ditemukan.\")\n",
        "\n",
        "# --- 2. Siapkan Gold Set & Query ---\n",
        "gold_set = {\n",
        "    \"putri AND malam\": {'01_kos_griya_cendekia.txt', '04_kos_amanah_putri.txt', '08_wisma_melati.txt', '13_omah_nyaman.txt'},\n",
        "    \"parkir AND mobil\": {'02_wisma_bintang_pelajar.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '10_kos_green_valley.txt'},\n",
        "    \"ac AND dalam\": {'01_kos_griya_cendekia.txt', '03_pondok_mahasiswa_jaya.txt', '04_kos_amanah_putri.txt', '05_graha_mandiri_residence.txt', '07_kos_eksklusif_emerald.txt', '09_kos_dahlia_putra.txt', '10_kos_green_valley.txt', '12_kos_simple_living.txt', '14_paviliun_cendana.txt'}\n",
        "}\n",
        "queries_to_test = {\n",
        "    \"putri AND malam\": \"kos putri yang ada jam malam\",\n",
        "    \"parkir AND mobil\": \"kos yang boleh parkir mobil\",\n",
        "    \"ac AND dalam\": \"kos AC kamar mandi dalam\"\n",
        "}\n",
        "k = 5\n",
        "all_metrics = {}\n",
        "\n",
        "# --- 3. Evaluasi Model 1: TF-IDF Standar ---\n",
        "print(\"\\n--- Model 1: TF-IDF Standar (Default) ---\")\n",
        "vectorizer_std = TfidfVectorizer(use_idf=True, smooth_idf=True, norm='l2')\n",
        "matrix_std = vectorizer_std.fit_transform(corpus)\n",
        "doc_ids = doc_ids_list\n",
        "\n",
        "metrics_std = []\n",
        "for bool_q, natural_q in queries_to_test.items():\n",
        "    results = vsm_ir.search_vsm(natural_q, vectorizer_std, matrix_std, doc_ids, top_k=k)\n",
        "    result_ids = [res['doc_id'] for res in results]\n",
        "    gs = gold_set.get(bool_q, set())\n",
        "\n",
        "    # Panggil dari eval_ir\n",
        "    map_k = eval_ir.calculate_map_at_k(result_ids, gs, k)\n",
        "\n",
        "    metrics_std.append(map_k)\n",
        "    print(f\"Query: '{natural_q}' -> MAP@{k}: {map_k:.4f}\")\n",
        "all_metrics[\"TF-IDF Standar\"] = np.mean(metrics_std)\n",
        "\n",
        "# --- 4. Evaluasi Model 2: TF-IDF Sublinear ---\n",
        "print(\"\\n--- Model 2: TF-IDF Sublinear (sublinear_tf=True) ---\")\n",
        "vectorizer_sub = TfidfVectorizer(use_idf=True, smooth_idf=True, norm='l2', sublinear_tf=True)\n",
        "matrix_sub = vectorizer_sub.fit_transform(corpus)\n",
        "print(f\"VSM (Sublinear TF) berhasil dibangun dengan shape: {matrix_sub.shape}\")\n",
        "\n",
        "metrics_sub = []\n",
        "for bool_q, natural_q in queries_to_test.items():\n",
        "    processed_query = vsm_ir.preprocess_query(natural_q)\n",
        "    query_vector = vectorizer_sub.transform([processed_query])\n",
        "    cosine_scores = vsm_ir.cosine_similarity(query_vector, matrix_sub)\n",
        "    scores = cosine_scores.flatten()\n",
        "    top_indices = np.argsort(scores)[-k:][::-1]\n",
        "\n",
        "    result_ids = []\n",
        "    for i in top_indices:\n",
        "        if scores[i] > 0:\n",
        "            result_ids.append(doc_ids[i])\n",
        "\n",
        "    gs = gold_set.get(bool_q, set())\n",
        "\n",
        "    # Panggil dari eval_ir\n",
        "    map_k = eval_ir.calculate_map_at_k(result_ids, gs, k)\n",
        "\n",
        "    metrics_sub.append(map_k)\n",
        "    print(f\"Query: '{natural_q}' -> MAP@{k}: {map_k:.4f}\")\n",
        "all_metrics[\"TF-IDF Sublinear\"] = np.mean(metrics_sub)\n",
        "\n",
        "# --- 5. Tampilkan Tabel Perbandingan ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"   PERBANDINGAN SKEMA PEMBOBOTAN (Rata-rata MAP@{k})\")\n",
        "print(\"=\"*50)\n",
        "for skema, rerata_map in all_metrics.items():\n",
        "    print(f\"  {skema:<20} : {rerata_map:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiVYOj2WZJc9"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQp9Sgp7cSf",
        "outputId": "edf1c833-8c83-4c85-ae5c-d2ad7ab60700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.120.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Instal library Gradio\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpFHwFAxarjb",
        "outputId": "aaef4ae8-ef35-4689-da21-5f98bb451fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key berhasil dimuat dengan aman dari Colab Secrets.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "\n",
        "print('API Key berhasil dimuat dengan aman dari Colab Secrets.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deYRrtRz2xaU",
        "outputId": "5a093786-fd5f-4f5b-df3c-eda282610fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app/chatbot.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/chatbot.py\n",
        "import os\n",
        "import gradio as gr\n",
        "import src.vsm_ir as vsm_ir # VSM Retriever\n",
        "import google.generativeai as genai # LLM Generator\n",
        "import argparse\n",
        "\n",
        "DATA_DIR = 'data/'\n",
        "\n",
        "# --- Fungsi retrieve_context\n",
        "def retrieve_context(raw_query, vectorizer, tfidf_matrix, doc_ids, top_k=3):\n",
        "    vsm_results = vsm_ir.search_vsm(raw_query, vectorizer, tfidf_matrix, doc_ids, top_k=top_k)\n",
        "    contexts = []\n",
        "    source_files = []\n",
        "    for res in vsm_results:\n",
        "        if res['score'] > 0.05:\n",
        "            try:\n",
        "                src_path = os.path.join(DATA_DIR, res['doc_id'])\n",
        "                with open(src_path, 'r', encoding='utf-8') as f:\n",
        "                    contexts.append(f.read())\n",
        "                source_files.append(res['doc_id'])\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "    return contexts, source_files\n",
        "\n",
        "# --- Fungsi generate_llm_response\n",
        "def generate_llm_response(query, contexts, llm_model):\n",
        "    if not contexts:\n",
        "        return \"Maaf, saya tidak menemukan informasi yang relevan dengan pertanyaan Anda.\"\n",
        "    context_str = \"\\n---\\n\".join(contexts)\n",
        "    system_prompt = (\n",
        "        \"Kamu adalah asisten Chatbot Kos yang ramah.\\n\"\n",
        "        \"Jawab pertanyaan pengguna HANYA berdasarkan konteks yang diberikan.\\n\"\n",
        "        \"JANGAN gunakan pengetahuan di luar konteks.\\n\"\n",
        "        \"Jika jawaban tidak ada di konteks, katakan 'Maaf, saya tidak menemukan informasi itu dalam data saya.'\\n\"\n",
        "        \"Jawab dalam Bahasa Indonesia.\"\n",
        "    )\n",
        "    prompt = f\"{system_prompt}\\n\\nKONTEKS:\\n{context_str}\\n\\nPERTANYAAN PENGGUNA:\\n{query}\\n\\nJAWABAN:\"\n",
        "    try:\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat memanggil LLM: {e}\") # Ini akan print error asli di log Colab\n",
        "        return \"Maaf, terjadi kesalahan saat memproses jawaban.\"\n",
        "\n",
        "print(\"Memuat model... Harap tunggu.\")\n",
        "MODELS_LOADED = False\n",
        "vectorizer = None\n",
        "tfidf_matrix = None\n",
        "doc_ids = None\n",
        "llm_model = None\n",
        "\n",
        "def load_all_models(api_key):\n",
        "    \"\"\"Fungsi helper untuk memuat semua model.\"\"\"\n",
        "    global vectorizer, tfidf_matrix, doc_ids, llm_model, MODELS_LOADED\n",
        "\n",
        "    try:\n",
        "        # 1. Muat Model VSM\n",
        "        vectorizer, tfidf_matrix, doc_ids = vsm_ir.build_vsm(DATA_DIR)\n",
        "        if vectorizer is None:\n",
        "            raise Exception(\"VSM Gagal dibangun.\")\n",
        "        print(\"Model VSM (Retrieval) berhasil dimuat.\")\n",
        "\n",
        "        # 2. Muat Model LLM\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API Key tidak diberikan.\")\n",
        "        genai.configure(api_key=api_key)\n",
        "        llm_model = genai.GenerativeModel('gemini-2.5-pro')\n",
        "        print(\"Model LLM (Generation) berhasil dimuat.\")\n",
        "        MODELS_LOADED = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"GAGAL memuat model: {e}\")\n",
        "        MODELS_LOADED = False\n",
        "\n",
        "def chat_interface(query):\n",
        "    if not MODELS_LOADED:\n",
        "        return \"Error: Model gagal dimuat. Periksa log.\"\n",
        "    contexts, sources = retrieve_context(query, vectorizer, tfidf_matrix, doc_ids, top_k=3)\n",
        "    answer = generate_llm_response(query, contexts, llm_model)\n",
        "    if sources:\n",
        "        answer += f\"\\n\\n(Sumber: {', '.join(sources)})\"\n",
        "    return answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Chatbot Kos RAG\")\n",
        "    parser.add_argument(\n",
        "        \"--api-key\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Google AI API Key\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Muat model menggunakan API key dari argumen\n",
        "    load_all_models(args.api_key)\n",
        "\n",
        "    if MODELS_LOADED:\n",
        "        # --- TEMA: PINK ---\n",
        "        theme = gr.themes.Soft(\n",
        "            primary_hue=\"pink\",\n",
        "            secondary_hue=\"pink\",\n",
        "            font=[gr.themes.GoogleFont(\"Inter\"), \"sans-serif\"]\n",
        "        ).set(\n",
        "            button_primary_background_fill=\"*primary_500\",\n",
        "            button_primary_background_fill_hover=\"*primary_400\",\n",
        "        )\n",
        "        examples = [\n",
        "           [\"kos putra bebas jam malam\"],\n",
        "            [\"kos putri yang ada AC dan kamar mandi dalam\"],\n",
        "            [\"berapa harga kos yang ada parkir mobil\"],\n",
        "            [\"kos khusus muslimah di semarang\"]\n",
        "        ]\n",
        "        # ---\n",
        "\n",
        "        iface = gr.Interface(\n",
        "            fn=chat_interface,\n",
        "            inputs=gr.Textbox(lines=3, placeholder=\"Ketik pertanyaan Anda tentang kos di sini...\"),\n",
        "            outputs=gr.Textbox(label=\"Jawaban Chatbot\", lines=15, show_copy_button=True),\n",
        "            title=\"🤖 Chatbot Pencari Kos\",\n",
        "            description=\"Chatbot RAG canggih yang menggunakan VSM (TF-IDF) untuk *Retrieval* dan Gemini 2.5 Pro untuk *Generation*.\",\n",
        "            theme=theme,\n",
        "            examples=examples,\n",
        "            allow_flagging=\"never\"\n",
        "        )\n",
        "        iface.launch(share=True)\n",
        "    else:\n",
        "        print(\"Gradio tidak dapat dijalankan karena model gagal dimuat.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qcxQZUD7tqo",
        "outputId": "245c9ce8-55a9-4f0e-a9c8-19829507cddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key berhasil dimuat dari Colab Secrets.\n",
            "Stemmer dan stopwords berhasil dimuat untuk preprocess.py.\n",
            "Memuat model... Harap tunggu.\n",
            "Membangun VSM dari data/...\n",
            "VSM (TF-IDF Matrix) berhasil dibangun dengan shape: (15, 213)\n",
            "Model VSM (Retrieval) berhasil dimuat.\n",
            "Model LLM (Generation) berhasil dimuat.\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://478cb46162a9352e1a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2958, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/drive/MyDrive/STKI-UTS-A11.2023.15373-Firnanda/app/chatbot.py\", line 125, in <module>\n",
            "    iface.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2865, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2962, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1153, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://478cb46162a9352e1a.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Menjalankan CHATBOT\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1. Ambil API Key dari Colab Secrets\n",
        "try:\n",
        "    # Pastikan nama di sini SAMA PERSIS dengan di Colab Secrets Anda\n",
        "    API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    if not API_KEY or API_KEY == \"\":\n",
        "        raise ValueError(\"API Key kosong atau tidak ditemukan.\")\n",
        "\n",
        "    print(\"API Key berhasil dimuat dari Colab Secrets.\")\n",
        "\n",
        "    # 2. Jalankan chatbot dan LEWATI API Key sebagai argumen\n",
        "    # Ini akan memperbaiki error Anda\n",
        "    !python -m app.chatbot --api-key \"{API_KEY}\"\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"GAGAL: Tidak bisa memuat API Key dari Colab Secrets.\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Pastikan Anda sudah menyimpan API Key dengan nama 'GOOGLE_API_KEY' di Colab Secrets (ikon 🔑).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guo5aFGTGnxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf091d9-dd09-47a9-e2e2-988bbc4fbf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmdir: failed to remove 'requirements.txt': Not a directory\n"
          ]
        }
      ],
      "source": [
        "!rmdir requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky4IeN0BGtq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f8752b-22fe-4c76-f661-98d79b78dc70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "nltk\n",
        "Sastrawi\n",
        "scikit-learn\n",
        "numpy\n",
        "gradio\n",
        "google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pK-F8oKHj3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2509694-2c03-449c-8da9-3b5c53908840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reports/readme.md\n"
          ]
        }
      ],
      "source": [
        "%%writefile reports/readme.md\n",
        "# Proyek Chatbot Kos - UTS Sistem Temu Kembali Informasi\n",
        "Proyek Chatbot Kos - UTS Sistem Temu Kembali Informasi\n",
        "\n",
        "Deskripsi Proyek\n",
        "\n",
        "Proyek ini merupakan submission untuk Ujian Tengah Semester (UTS) mata kuliah Sistem Temu Kembali Informasi (STKI).\n",
        "Tujuannya adalah membangun sebuah chatbot pencari kos berbasis arsitektur RAG (Retrieval-Augmented Generation) yang mampu menjawab pertanyaan pengguna secara informatif. Sistem ini menggunakan korpus 15 dokumen .txt yang berisi data kos fiktif.\n",
        "\n",
        "Nama: Firnanda Rahmawati\n",
        "NIM: A11.2023.15373\n",
        "Mata Kuliah: A11.4706 - Sistem Temu Kembali Informasi\n",
        "\n",
        "Teknologi yang Digunakan\n",
        "\n",
        "Model Retrieval:\n",
        "\n",
        "1. Boolean (Soal 3): Menggunakan Inverted Index untuk pencarian eksak (AND, OR, NOT).\n",
        "\n",
        "2. Vector Space Model (Soal 4): Menggunakan TF-IDF dan Cosine Similarity untuk pencarian relevansi (ranking).\n",
        "\n",
        "3. Preprocessing (Soal 2):\n",
        "Tokenisasi, Case Folding, Stopword Removal (nltk), dan Stemming (Sastrawi).\n",
        "\n",
        "4. Model Generation (Soal 5):\n",
        "Google Gemini API (gemini-1.5-flash) untuk menghasilkan jawaban yang natural berdasarkan konteks.\n",
        "Antarmuka (UI) (Soal 5):\n",
        "Gradio untuk membuat antarmuka web chatbot yang interaktif.\n",
        "Orkestrasi:\n",
        "Python (file modul di src/)\n",
        "Google Colab (.ipynb) untuk pengembangan, pengujian, dan evaluasi.\n",
        "\n",
        "Struktur Folder\n",
        "\n",
        "Struktur folder proyek ini mengikuti ketentuan yang diberikan dalam soal ujian:\n",
        "\n",
        "/\n",
        "├── data/                 # Berisi 15 file .txt data kos (korpus asli)\n",
        "├── src/                  # Berisi modul-modul Python\n",
        "│   ├── preprocess.py       # (Soal 2: Fungsi preprocessing)\n",
        "│   ├── boolean_ir.py     # (Soal 3: Model Boolean & eval)\n",
        "│   ├── vsm_ir.py         # (Soal 4: Model VSM, TF-IDF, eval)\n",
        "│   └── search_engine.py  # (Soal 5: CLI Orchestrator)\n",
        "├── app/\n",
        "│   └── chatbot.py        # (Soal 5: Chatbot RAG dengan Gradio & Gemini)\n",
        "├── notebooks/\n",
        "│   └── UTS_STKI_<nim>.ipynb # Notebook utama untuk demo & evaluasi\n",
        "├── reports/\n",
        "│   └── laporan.pdf         # Laporan analisis proyek\n",
        "├── readme.md               # (File ini)\n",
        "└── requirements.txt        # Daftar library yang dibutuhkan\n",
        "\n",
        "\n",
        "Panduan Setup dan Instalasi\n",
        "\n",
        "Install Kebutuhan Library:\n",
        "\n",
        "pip install -r requirements.txt\n",
        "\n",
        "\n",
        "(Catatan: nltk mungkin memerlukan unduhan data tambahan)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "Setup API Key (PENTING):\n",
        "Proyek ini membutuhkan API Key dari Google AI Studio (Gemini).\n",
        "\n",
        "Lokal: Simpan API Key Anda sebagai environment variable dengan nama GOOGLE_API_KEY.\n",
        "\n",
        "Colab: Simpan API Key Anda di Colab Secrets (ikon 🔑) dengan nama GOOGLE_API_KEY.\n",
        "\n",
        "Cara Menjalankan\n",
        "\n",
        "1. (Disarankan) Melalui Notebook (Google Colab)\n",
        "\n",
        "Cara termudah untuk mereproduksi hasil adalah dengan membuka file notebooks/UTS_STKI_A11.2023.15373.ipynb di Google Colab.\n",
        "\n",
        "Unggah seluruh folder proyek ke Google Drive Anda.\n",
        "\n",
        "Buka file .ipynb di Colab dan mount Google Drive Anda.\n",
        "\n",
        "Pastikan Anda telah memasukkan GOOGLE_API_KEY Anda di Colab Secrets (ikon 🔑).\n",
        "\n",
        "Jalankan semua sel dari atas ke bawah secara berurutan.\n",
        "\n",
        "Sel terakhir akan menjalankan !python -m app.chatbot --api-key \"...\" dan memberikan link publik Gradio yang dapat Anda buka.\n",
        "\n",
        "2. Menjalankan Chatbot Secara Lokal (via Terminal)\n",
        "\n",
        "Pastikan Anda sudah melakukan Panduan Setup (Langkah 1-4 di atas).\n",
        "\n",
        "Pastikan Anda sudah mengatur GOOGLE_API_KEY sebagai environment variable.\n",
        "\n",
        "Jalankan aplikasi chatbot dari direktori utama proyek:\n",
        "\n",
        "# (Ganti \"API_KEY_ANDA\" dengan key Anda jika tidak menggunakan env variable)\n",
        "python -m app.chatbot --api-key \"API_KEY_ANDA\"\n",
        "\n",
        "\n",
        "Buka link lokal (http://127.0.0.1:....) yang muncul di terminal Anda."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAH3fuDfAXYCeqOx8pJ7i1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}